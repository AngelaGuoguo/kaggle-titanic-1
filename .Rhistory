install.packages('plyr')
install.packages('lubridate')
install.packages(RMySQL)
install.packages('RMySQL')
install.packages('reshape2')
setwd("~/kaggle-titanic")
train_error <- function(survived_pred) {
# Check to see which predictions our model gets wrong
which(train$survived_pred != train$survived)
# Calculate our % accuracy on the TRAIN data set
((length(which(train$survived_pred == train$survived))) /
length(train$survived)) * 100
}
# Goal:     (1) Construct basic randomForest models from the data
#           (2) Select the best model (Model selection)
#           (3) Save a prediction with our best randomForest
library(randomForest)
library(plyr)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create randomForest model
###
# Create random forest based on PCLASS, SEX, FARE, and AGE
forest <- randomForest(survived ~ sex.name + pclass + age + fare.distance + fare,
data = train, ntree = 15000, importance = TRUE)
summary(forest)
# Extract the importance of each variable
importance(forest)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(forest, train)
# Make our prediction on the TEST data set
test$survived <- predict(forest, test)
# save csv file for submission
write.csv(test, "Submissions/randomForest-04.csv")
train_error(survived_pred)
# Goal: construct baisc Support vector Machine model
library(kernlab)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create SVM model
###
# Create the SVM model with SEX, PCLASS, FARE, and AGE
svm.model <- ksvm(survived ~ sex.name + pclass + age + fare, data = train)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(svm.model, train, type = "response")
# Make our prediction on the TEST data set
test$survived <- predict(svm.model, test, type = "response")
# save csv file for submission
write.csv(test, "Submissions/svm-model-04.csv")
train_error(survived_pred)
# Goal: (1) Construct basic Probit models from the data
library(plyr)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create probit model and make prediction
###
# Create probit with SEX, PCLASS, FARE, and AGE
probit <- glm(survived ~ sex.name + pclass + age + fare.distance + fare, data = train,
family = binomial(link = "probit"))
summary(probit)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(probit, train, type = "response")
# Make a prediction with our probit on TEST
test$survived <- predict(probit, test, type = "response")
test$survived[test$survived >= 0.5] <- 1
test$survived[test$survived < 0.5] <- 0
# save csv file for submission
write.csv(test, "Submissions/probit-04.csv")
train_error(survived_pred)
predict(probit, train, type = "response")
train$survived_pred[train$survived >= 0.5] <- 1
train$survived_pred[train$survived < 0.5] <- 0
# Goal: (1) Construct basic Probit models from the data
library(plyr)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create probit model and make prediction
###
# Create probit with SEX, PCLASS, FARE, and AGE
probit <- glm(survived ~ sex.name + pclass + age + fare.distance + fare, data = train,
family = binomial(link = "probit"))
summary(probit)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(probit, train, type = "response")
train$survived_pred[train$survived_pred >= 0.5] <- 1
train$survived_pred[train$survived_pred < 0.5] <- 0
# Make a prediction with our probit on TEST
test$survived <- predict(probit, test, type = "response")
test$survived[test$survived >= 0.5] <- 1
test$survived[test$survived < 0.5] <- 0
# save csv file for submission
write.csv(test, "Submissions/probit-04.csv")
train_error(survived_pred)
randomForest(survived ~ sex + pclass + fare, data = train_new)
?ntree
model <- "randomForest(survived ~ sex + pclass + fare, data = train_new)"
model
cv_5folds <- function("model") {
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# When we change our model, we need to edit this!!!
temp_model <- randomForest(survived ~ sex + pclass + fare, data = train_new)
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_5folds <- function("model") {
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# When we change our model, we need to edit this!!!
temp_model <- randomForest(survived ~ sex + pclass + fare, data = train_new)
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_5folds <- function(model) {
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# When we change our model, we need to edit this!!!
temp_model <- randomForest(survived ~ sex + pclass + fare, data = train_new)
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
model
parse(randomForest(survived ~ sex + pclass + fare, data = train_new))
parse(model)
parse(text = model)
eval(parse(text = model))
model
?str_replace
library(stringr)
?str_replace
model
str_replace(model, "train", "train_new")
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
# Load in our models (edit this!)
source("2-randomForest.R")
str_replace(model, "data = train", "data = train_new")
model <- "randomForest(survived ~ sex.name + pclass + age + fare.distance + fare, data = train, ntree = 5000, importance = TRUE)"
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
# Load in our models (edit this!)
source("2-randomForest.R")
str_replace(model, "data = train", "data = train_new")
cv_5folds <- function(model) {
# Fix our model input
model <- str_replace(model, "data = train", "data = train_new")
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# Train our model on the new, smaller training data set
temp_model <- eval(parse(text = model))
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_5folds(model)
cv_kfolds <- function(model, k = 5) {
# Fix our model input
model <- str_replace(model, "data = train", "data = train_new")
# Set number of equally-sized, non-overlapping chunks
k <- k
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# Train our model on the new, smaller training data set
temp_model <- eval(parse(text = model))
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_kfolds(model)
cv_kfolds(model, k = 2)
source("2-randomForest.R")
train_error(survived_pred)
cv_kfolds(model, k = 11)
source("3-SVM.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
# Probit
source("4-probit.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
# Probit
source("4-probit.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
source("4-probit.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
