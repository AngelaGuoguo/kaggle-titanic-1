<<<<<<< HEAD
fix(bacon_polarity)
bacon_polarity <- as.data.frame(bacon_polarity[, 4])
bacon_polarity <- classify_polarity(bacon_tweets)
bacon_polarity <- as.data.frame(bacon_polarity[, 4])
bacon_polarity$tag <- "bacon"
?merge
?matrix
bacon_polarity <- classify_polarity(bacon_tweets)
bacon_polarity.new <- matrix(nrow = 1500, ncol = 2)
bacon_polarity.new[1:1500, 1] <- bacon.polarity[, 4]
bacon_polarity.new[1:1500, 2] <- "bacon"
rstudio::viewData(bacon_polarity.new)
bacon_polarity.new[1:1500, 1] <- bacon_polarity[, 4]
bacon_polarity.new[1:1500, 2] <- "bacon"
rstudio::viewData(bacon_polarity)
rstudio::viewData(bacon_polarity.new)
yolo_polarity <- classify_polarity(yolo_tweets)
yolo_polarity.new <- matrix(nrow = 1500, ncol = 2)
yolo_polarity.new[1:1500, 1] <- yolo_polarity[, 4]
yolo_polarity.new[1:1500, 2] <- "yolo"
fml_polarity <- classify_polarity(fml_tweets)
fml_polarity.new <- matrix(nrow = 1500, ncol = 2)
fml_polarity.new[1:1500, 1] <- fml_polarity[, 4]
fml_polarity.new[1:1500, 2] <- "fml"
blessed_polarity <- classify_polarity(blessed_tweets)
blessed_polarity.new <- matrix(nrow = 1500, ncol = 2)
blessed_polarity.new[1:1500, 1] <- blessed_polarity[, 4]
blessed_polarity.new[1:1500, 2] <- "blessed"
polarities <- rbind(yolo_polarity.new, fml_polarity.new,
blessed_polarity.new, bacon_polarity.new)
colnames(polarities) <- c("Polarities", "Hashtag")
qplot(tag, fill = polarities, data = polarities)
polarities$tag
qplot(polarities[, 2], fill = polarities[, 1])
?reorder
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(~cut)
library(ggplot2)
library(plyr)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(~cut)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(cut ~ clarity)
warnings()
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(clarity~)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(clarity~)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(clarity~.)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(~clarity)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .5)) + facet_wrap(~cut)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(~cut)
less_than_1.5 <- subset(diamonds, carat <= 1.5)
data(diamonds)
less_than_1.5 <- subset(diamonds, carat <= 1.5)
less_than_1.5 <- subset(diamonds, carat <= 1)
less_than_1 <- subset(diamonds, carat < 1)
less_than_1.5 <- subset(diamonds, carat <= 1.5)
qplot(carat, price, aes(shape = I("."), data = diamonds)
qplot(carat, price, aes(shape = I("."), data = diamonds))
qplot(carat, price, data = diamonds,
f
da
qplot(carat, price, aes(shape = I("."), data = diamonds)
qplot(carat, price, aes(shape = I("."), data = diamonds))
qplot(carat, price, aes(shape = I("."), data = diamonds))
qplot(carat, price, aes(shape = I("."), data = diamonds))
rstudio::viewData(diamonds)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(~cut)
qplot(carat, price, aes(shape = I("."), data = diamonds))
qplot(carat, price, data = diamonds))
qplot(carat, price, data = diamonds)
qplot(carat, price, aes(shape = I(".")),
geom = "point", data = diamonds)
qplot(carat, price, data = diamonds)
qplot(carat, price, data = diamonds, geom = "boxplot")
qplot(carat, price, data = diamonds, geom = "boxplot",
group = round_any(carat, .25))
qplot(carat, price, data = diamonds, geom = "smooth")
qplot(carat, price, data = diamonds, geom = "lm")
qplot(carat, price, data = diamonds)
qplot(carat, price, color = cut, data = diamonds)
qplot(carat, price, data = diamonds, geom = "boxplot", color = cut
group = round_any(carat, .25))
qplot(carat, price, data = diamonds, geom = "boxplot", color = cut,
group = round_any(carat, .25))
qplot(carat, price, data = diamonds, geom = "boxplot",
group = round_any(carat, .25))
qplot(carat, price, data = diamonds, geom = "boxplot") + facet_wrap(~cut)
less_than_1.5 <- subset(diamonds, carat <= 1.5)
qplot(carat, price, data = less_than_1.5)
qplot(cut, data = less_than_1.5)
greater_than_1.5 <- subset(diamonds, carat > 1.5)
qplot(cut, data = less_than_1.5)
qplot(cut, data = greater_than_1.5)
library(gridExtra)
p1 <- qplot(cut, data = less_than_1.5)
p2 <- qplot(cut, data = greater_than_1.5)
gridarrange(p1, p2)
grid.arrange(p1, p2)
?grid.arrange
grid.arrange(p1, p2, nrow = 1)
qplot(cut, ..density.., data = less_than_1.5)
qplot(cut, ..density.., geom = "freqpoly", data = less_than_1.5)
qplot(cut, data = less_than_1.5)
qplot(cut, data = less_than_1.5) + scale_y_continous(0,2000)
qplot(cut, data = greater_than_1.5) + scale_y_continuous(0, 2000)
qplot(cut, data = less_than_1.5) + scale_y_continuous(0,20000)
qplot(cut, data = greater_than_1.5) + scale_y_continuous(0, 20000)
qplot(cut, data = less_than_1.5) + scale_y_discrete(0,20000)
qplot(cut, data = greater_than_1.5) + scale_y_discrete(0, 20000)
qplot(cut, data = less_than_1.5) + ylim(0,25000)
qplot(cut, data = greater_than_1.5) + ylim(0, 25000)
p1 <- qplot(cut, data = less_than_1.5) + ylim(0,25000)
p2 <- qplot(cut, data = greater_than_1.5) + ylim(0, 25000)
grid.arrange(p1, p2, nrow = 1)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(~cut)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot") + facet_wrap(~clarity)
rstudio::viewData(less_than_1.5)
?clarity
?diamonds
g <- lm(price ~ ., data = diamonds)
summary(g)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot") + facet_wrap(~color)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(~cut)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(color~cut)
?diamonds
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(color~cut)
qplot(carat, price, data = greater_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(color~cut)
qplot(cut, data = greater_than_1.5, fill = color)
qplot(cut, data = less_than_1.5, fill = color)
qplot(carat, price, color = cut, data = diamonds)
qplot(carat, price, data = diamonds, geom = "boxplot") + facet_wrap(~cut)
qplot(carat, price, data = diamonds,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(~cut)
qplot(carat, price, color = cut, data = diamonds)
qplot(carat, price, data = diamonds, geom = "boxplot") + facet_wrap(~cut)
qplot(carat, price, data = diamonds, geom = "boxplot", group = round_any(carat, .25))
p2 <- qplot(cut, geom = "freqpoly", data = greater_than_1.5) + ylim(0, 25000)
p2
p1 <- qplot(cut, data = less_than_1.5) + ylim(0,25000)
p2 <- qplot(cut, data = greater_than_1.5) + ylim(0, 25000)
grid.arrange(p1, p2, nrow = 1)
p1 <- qplot(cut, data = less_than_1.5) + ylim(0,25000) + ggtitle("Less than 1.5")
p2 <- qplot(cut, data = greater_than_1.5) + ylim(0, 25000) + ggtitle("Greater than 1.5")
grid.arrange(p1, p2, nrow = 1)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(color~cut)
qplot(carat, price, data = greater_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(color~cut)
qplot(carat, price, data = less_than_1.5,
geom = "boxplot", group = round_any(carat, .25)) + facet_wrap(color~cut)
data(mpg)
rstudio::viewData(mpg)
?mpg
mpg.1999 <- subset(mpg, year == 1999)
mpg.2008 <- subset(mpg, year == 2008)
qplot(hwy, data = mpg, binwidth = 1,
geom = "freqpoly", colour = year)
qplot(hwy, ..density.., data = mpg, binwidth = 500,
geom = "freqpoly", colour = year)
rstudio::viewData(mpg)
qplot(displ, ..density.., data = mpg, binwidth = 500,
geom = "freqpoly", colour = year)
qplot(manufacturer, ..density.., data = mpg, binwidth = 500,
geom = "freqpoly", colour = year)
qplot(hwy, ..density.., data = mpg, binwidth = 500,
geom = "freqpoly", colour = year)
qplot(cty, ..density.., data = mpg, binwidth = 500,
geom = "freqpoly", colour = year)
qplot(hwy, ..density.., data = mpg, binwidth = .1,
geom = "freqpoly", colour = year)
qplot(hwy, ..density.., data = mpg, binwidth = .01,
geom = "freqpoly", colour = year)
qplot(hwy, ..density.., data = mpg,
geom = "freqpoly", colour = year)
qplot(hwy, ..density.., data = mpg, colour = year)
qplot(hwy, ..density.., data = mpg, fill = year)
qplot(hwy, data = mpg, fill = year)
qplot(hwy, data = mpg)
qplot(hwy, color = year, geom = "freqpoly", data = mpg)
qplot(hwy, color = year, data = mpg)
qplot(hwy, geom = "freqpoly", data = mpg)
qplot(hwy, geom = "freqpoly", color = year, data = mpg)
qplot(hwy, geom = "freqpoly", data = mpg.1999) + qplot(hwy, geom = "freqpoly", data = mpg.2008)
ggplot(histogram, aes(x = mpg, color = I("red")), data = mpg.1999) + ggplot(histogram, aes(x = mpg, color = I("blue")), data = mpg.2008)
ggplot(aes(x = mpg, color = I("red")), data = mpg.1999) + ggplot(aes(x = mpg, color = I("blue")), data = mpg.2008)
qplot(year, hwy, geom = "boxplot", data = mpg) + facet_wrap(~class)
qplot(as.character(year), hwy, geom = "boxplot", data = mpg) + facet_wrap(~class)
rstudio::viewData(diamonds)
qplot(as.character(color), as.character(cut), geom = "boxplot", data = diamonds)
qplot(as.character(color), as.character(cut), data = diamonds)
qplot(as.character(color), price, geom = "boxplot", data = mpg)
qplot(as.character(color), price, geom = "boxplot", data = diamonds)
qplot(as.character(color), price, geom = "boxplot", data = diamonds) + facet_wrap(~cut)
qplot(table, depth, data = diamonds)
qplot(displ, hwy, data = mpg) + facet_wrap(~cyl)
qplot(as.charater(cyl), hwy, data = mpg)
qplot(as.character(cyl), hwy, data = mpg)
qplot(as.character(cyl), hwy, geom = "boxplot", data = mpg)
qplot(as.character(cyl), hwy, geom = "boxplot", data = mpg) + facet_wrap(~class)
qplot(as.character(cyl), hwy, geom = "boxplot", data = mpg)
qplot(class, hwy, geom = "boxplot", data = mpg)
qplot(carat, price, data = greater_than_1.5)
qplot(color, price, data = greater_than_1.5, geom = "boxplot")
qplot(color, price, data = greater_than_1.5, geom = "boxplot") + facet_wrap(~cut)
qplot(carat, price, data = greater_than_1.5)
qplot(color, price, data = greater_than_1.5, geom = "boxplot")
qplot(color, price, data = less_than_1.5)
last_plot() + facet_wrap(~cut)
qplot(color, price, data = less_than_1.5, geom = "boxplot")
qplot(cut, price, data = greater_than_1.5, geom = "boxplot") + facet_wrap(~cut)
qplot(cut, price, data = greater_than_1.5, geom = "boxplot") + facet_wrap(~color)
qplot(cut, price, data = greater_than_1.5, geom = "boxplot")
qplot(cut, price, data = greater_than_1.5, geom = "boxplot") + facet_grid(~color)
ideal_diamonds <- subset(diamonds, cut == "Ideal")
qplot(carat, price, data = idea_diamonds)
qplot(carat, price, data = ideal_diamonds)
qplot(color, price, data = ideal_diamonds, geom = "boxplot")
qplot(color, price, data = ideal_diamonds, geom = "boxplot") + facet_wrap(~clarity)
awesome_diamonds <- subset(diamonds, cut == "Ideal" & color == "D")
qplot(carat, price, data = awesome_diamonds)
qplot(log(carat), log(price), data = awesome_diamonds)
qplot(carat, price, data = awesome_diamonds)
qplot(clarity, price, data = awesome_diamonds)
qplot(clarity, price, data = awesome_diamonds, geom = "boxplot")
?diamonds
summary(awesome_diamonds)
qplot(table, depth, data = awesome_diamonds, geom = "boxplot")
qplot(table, depth, data = awesome_diamonds)
qplot(table, depth, data = diamonds)
qplot(x, y, data = awesome_diamonds)
qplot(x, y, data = diamonds)
qplot(x, y, data = awesome_diamonds)
mpg.manual <- subset(mpg, trans == "manual(m5)" | trans == "manual(m6)")
mpg.manual <- subset(mpg, trans == "manual(m5)" | trans == "manual(m6)")
mpg.auto <- subset(mpg, trans != "manual(m5)" & trans != "manual(m6)")
qplot(hwy, data = mpg.manual)
qplot(hwy, data = mpg.auto)
qplot(hwy, ..density.., data = mpg.manual)
qplot(hwy, ..density.., data = mpg.manual, geom = "freqpoly")
qplot(hwy, ..density.., data = mpg.auto, geom = "freqpoly")
qplot(hwy, ..density.., data = mpg, geom = "freqpoly",
colour = trans)
if (mpg$trans == "manual(m5)" | trans == "manual(m6)") {
mpg$trans_clean <- "manual"
else
mpg$trans_clean <- "auto"
if (mpg$trans == "manual(m5)" | trans == "manual(m6)") {
mpg$trans_clean <- "manual"
else {
mpg$trans_clean <- "auto"
}
if (mpg$trans == "manual(m5)" | trans == "manual(m6)") {
mpg$trans_clean <- "manual"
}
if (mpg$trans == "manual(m5)" | mpg$trans == "manual(m6)") {
mpg$trans_clean <- "manual"
}
q <- qplot(hwy, ..density.., data = mpg.manual, geom = "freqpoly") + geom_point()
p <- qplot(hwy, ..density.., data = mpg.auto, geom = "freqpoly")
q + p
q <- qplot(hwy, ..density.., data = mpg.manual, geom = "freqpoly") + geom_point()
q + geom_point(hwy, ..density.., data = mpg.auto, geom = "freqpoly")
q <- ggplot(hwy, ..density.., data = mpg.manual, geom = "freqpoly") + geom_point()
q <- ggplot(aes(x = hwy), ..density.., data = mpg.manual, geom = "freqpoly") + geom_point()
q
q <- ggplot(aes(x = hwy, y = ..density..), data = mpg.manual, geom = "freqpoly") + geom_point()
q
p1 <- qplot(hwy, ..density.., data = mpg.manual, geom = "freqpoly")
p2 <- qplot(hwy, ..density.., data = mpg.auto, geom = "freqpoly")
grid.arrange(p1, p2, nrow = 1)
p1 <- qplot(hwy, ..density.., data = mpg.manual, geom = "freqpoly") + ggtitle("Manual Cars")
p2 <- qplot(hwy, ..density.., data = mpg.auto, geom = "freqpoly") + ggtitle("Automatic Cars")
grid.arrange(p1, p2, nrow = 1)
qplot(hwy, class, data = mpg)
binarize <- function(x) {
x <- x
bin <- NULL
while (x  > 0)
{
bin <- paste0(x %% 2, bin, collapse = NULL)
x <- floor(x / 2)
}
return(bin)
}
binarize(192)
?rand
?unif
?runif
runif(4)
run(1)
runif(50)
dunif(50)
dunif(3)
dunif(1)
dunif(2)
dunif(.5)
dunif(.2)
dunif(-1)
punif(5)
qunif(5)
qunif(.5)
qunif(.3)
qunif(2/3)
z = [0 0]
z = [0, 0]
z = c(0, 0)
?matrix
library(ggplot2)
z <- c(0, 0)
g <- ggplot()
for (i in 1:100000) {
r <- runif(1)
if (r < .01) {
z <- matrix(0, 0, 0, .16, nrow = 2, byrow = T) * z
}
else if (r < .86) {
z <- matrix(.85, .04, -.04, .85, nrow = 2, byrow = T) * z
}
else if (r < .93) {
z <- matrix(0.2, -.26, .23, .22, nrow = 2, byrow = TRUE) * z + c(0, 1.6)
}
else {
z <- matrix(-.15, .28, .26, .24, nrow = 2, byrow = TRUE) * z + c(0, .44)
}
g + geom_point(x = z[1], y = z[2])
}
library(ggplot2)
z <- c(0, 0)
g <- ggplot()
for (i in 1:100000) {
r <- runif(1)
if (r < .01) {
z <- matrix(0, 0, 0, .16, nrow = 2, byrow = T) * z
}
else if (r < .86) {
z <- matrix(.85, .04, -.04, .85, nrow = 2, byrow = T) * z
}
else if (r < .93) {
z <- matrix(0.2, -.26, .23, .22, nrow = 2, byrow = T) * z + c(0, 1.6)
}
else {
z <- matrix(-.15, .28, .26, .24, nrow = 2, byrow = T) * z + c(0, .44)
}
g <- g + geom_point(x = z[1], y = z[2])
}
ggplot() + geom_point(x = 1, y = 2)
ggplot() + geom_point(aes(x = 1, y = 2))
library(ggplot2)
z <- c(0, 0)
g <- ggplot()
for (i in 1:100000) {
r <- runif(1)
if (r < .01) {
z <- matrix(0, 0, 0, .16, nrow = 2, byrow = T) * z
}
else if (r < .86) {
z <- matrix(.85, .04, -.04, .85, nrow = 2, byrow = T) * z
}
else if (r < .93) {
z <- matrix(0.2, -.26, .23, .22, nrow = 2, byrow = T) * z + c(0, 1.6)
}
else {
z <- matrix(-.15, .28, .26, .24, nrow = 2, byrow = T) * z + c(0, .44)
}
g <- g + geom_point(aes(x = z[1], y = z[2]))
}
g
r
x <- matrix(0.2, -.26, .23, .22, nrow = 2, byrow = T)
matrix(0.2, -.26, .23, .22, nrow = 2, byrow = T)
?matrix
matrix(0, 0, 0, .16, nrow = 2, byrow = T)
matrix(c(0, 0, 0, .16), nrow = 2, byrow = T)
library(ggplot2)
z <- c(0, 0)
g <- ggplot()
for (i in 1:100000) {
r <- runif(1)
if (r < .01) {
z <- matrix(c(0, 0, 0, .16), nrow = 2, byrow = T) * z
}
else if (r < .86) {
z <- matrix(c(.85, .04, -.04, .85), nrow = 2, byrow = T) * z
}
else if (r < .93) {
z <- matrix(c(0.2, -.26, .23, .22), nrow = 2, byrow = T) * z + c(0, 1.6)
}
else {
z <- matrix(c(-.15, .28, .26, .24), nrow = 2, byrow = T) * z + c(0, .44)
}
g <- g + geom_point(aes(x = z[1], y = z[2]))
}
g
z
z <- matrix(c(0, 0), nrow = 2)
g <- ggplot()
for (i in 1:100000) {
r <- runif(1)
if (r < .01) {
z <- matrix(c(0, 0, 0, .16), nrow = 2, byrow = T) * z
}
else if (r < .86) {
z <- matrix(c(.85, .04, -.04, .85), nrow = 2, byrow = T) * z
}
else if (r < .93) {
z <- matrix(c(0.2, -.26, .23, .22), nrow = 2, byrow = T) * z + c(0, 1.6)
}
else {
z <- matrix(c(-.15, .28, .26, .24), nrow = 2, byrow = T) * z + c(0, .44)
}
g <- g + geom_point(aes(x = z[1], y = z[2]))
}
z
matrix(c(1, 2, 3, 4)) * z
matrix(c(1, 2, 3, 4))
matrix(c(1, 2, 3, 4), nrow = 2)
matrix(c(1, 2, 3, 4), nrow = 2) * z
matrix(c(1, 2, 3, 4), nrow = 2) * matrix(c(0, 0), nrow = 1)
matrix(c(1, 2, 3, 4), nrow = 2) * matrix(c(0, 0), nrow = 2)
library(ggplot2)
z <- matrix(c(0, 0), nrow = 2)
g <- ggplot()
for (i in 1:100000) {
r <- runif(1)
if (r < .01) {
z <- matrix(c(0, 0, 0, .16), nrow = 2, byrow = T) %*% z
}
else if (r < .86) {
z <- matrix(c(.85, .04, -.04, .85), nrow = 2, byrow = T) %*% z
}
else if (r < .93) {
z <- matrix(c(0.2, -.26, .23, .22), nrow = 2, byrow = T) %*% z + c(0, 1.6)
}
else {
z <- matrix(c(-.15, .28, .26, .24), nrow = 2, byrow = T) %*% z + c(0, .44)
}
g <- g + geom_point(aes(x = z[1], y = z[2]))
}
g
z
g + geom_point(aes(x = z[1], y = z[2]))
library(gbm)
library(plyr)
R.Version()
install.packages("installr")
updateR(install_R = TRUE)
library(installr)
updateR(install_R = TRUE)
library(gbm)
library(plyr)
gbm3 <- gbm(survived ~ sex.name + pclass + fare + age,
data = train)
load("Data/train_clean.RData")  # 891 obs
setwd("C:/Ruby193/_site/kaggle-titanic")
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
gbm3 <- gbm(survived ~ sex.name + pclass + fare + age,
data = train)
gbm <- gbm(survived ~ sex.name + pclass + fare.distance + age,
data = train, distribution = "adaboost", n.trees = 1000)
gbm2 <- gbm(survived ~ sex.name + pclass + fare + age,
data = train, distribution = "multinomial", n.trees = 1000)
test2 <- test
test2$survived <- predict(gbm2, test2, n.trees = 500, type = "response")
rstudio::viewData(test2)
rstudio::viewData(test2)
gbm <- gbm(survived ~ sex.name + pclass + fare.distance + age,
data = train, distribution = "adaboost", n.trees = 1000)
test$survived <- predict(gbm, test, n.trees = 500, type = "link")
test$survived <- plogis(test$survived)
rstudio::viewData(test)
test$survived <- predict(gbm, test, n.trees = 500, type = "link")
rstudio::viewData(test2)
rstudio::viewData(test)
test$survived <- predict(gbm, test, n.trees = 500, type = "link")
test$survived[which(test$survived < 1)] <- 0
test$survived[which(test$survived >= 1)] <- 1
write.csv(test, "Submissions/gbm-02.csv")
highest <- read.csv("Submissions/highest.csv")
which(test$survived != highest$survived)
rstudio::viewData(train)
net4 <- nnet(survived ~ sex.name + pclass + fare + age, data = train, size = 2,
linout = FALSE, maxit = 10000)
library(nnet)
net4 <- nnet(survived ~ sex.name + pclass + fare + age, data = train, size = 2,
linout = FALSE, maxit = 10000)
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
result4 <- predict(net4, test, type = "class")
net4 <- nnet(survived ~ sex.name + pclass + fare + age, data = train, size = 2,
linout = FALSE, maxit = 10000)
result4 <- predict(net4, test, type = "class")
fix(result4)
test.net4 <- test
test.net4$survived <- result4
write.csv(test.net4, "Submissions/neuralnet-03.csv")
which(test.net4$survived != highest$survived)
highest <- read.csv("Submissions/highest.csv")
which(test.net4$survived != highest$survived)
=======
install.packages('plyr')
install.packages('lubridate')
install.packages(RMySQL)
install.packages('RMySQL')
install.packages('reshape2')
setwd("~/kaggle-titanic")
train_error <- function(survived_pred) {
# Check to see which predictions our model gets wrong
which(train$survived_pred != train$survived)
# Calculate our % accuracy on the TRAIN data set
((length(which(train$survived_pred == train$survived))) /
length(train$survived)) * 100
}
# Goal:     (1) Construct basic randomForest models from the data
#           (2) Select the best model (Model selection)
#           (3) Save a prediction with our best randomForest
library(randomForest)
library(plyr)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create randomForest model
###
# Create random forest based on PCLASS, SEX, FARE, and AGE
forest <- randomForest(survived ~ sex.name + pclass + age + fare.distance + fare,
data = train, ntree = 15000, importance = TRUE)
summary(forest)
# Extract the importance of each variable
importance(forest)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(forest, train)
# Make our prediction on the TEST data set
test$survived <- predict(forest, test)
# save csv file for submission
write.csv(test, "Submissions/randomForest-04.csv")
train_error(survived_pred)
# Goal: construct baisc Support vector Machine model
library(kernlab)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create SVM model
###
# Create the SVM model with SEX, PCLASS, FARE, and AGE
svm.model <- ksvm(survived ~ sex.name + pclass + age + fare, data = train)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(svm.model, train, type = "response")
# Make our prediction on the TEST data set
test$survived <- predict(svm.model, test, type = "response")
# save csv file for submission
write.csv(test, "Submissions/svm-model-04.csv")
train_error(survived_pred)
# Goal: (1) Construct basic Probit models from the data
library(plyr)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create probit model and make prediction
###
# Create probit with SEX, PCLASS, FARE, and AGE
probit <- glm(survived ~ sex.name + pclass + age + fare.distance + fare, data = train,
family = binomial(link = "probit"))
summary(probit)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(probit, train, type = "response")
# Make a prediction with our probit on TEST
test$survived <- predict(probit, test, type = "response")
test$survived[test$survived >= 0.5] <- 1
test$survived[test$survived < 0.5] <- 0
# save csv file for submission
write.csv(test, "Submissions/probit-04.csv")
train_error(survived_pred)
predict(probit, train, type = "response")
train$survived_pred[train$survived >= 0.5] <- 1
train$survived_pred[train$survived < 0.5] <- 0
# Goal: (1) Construct basic Probit models from the data
library(plyr)
# Load in the cleaned data sets
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
###
### Create probit model and make prediction
###
# Create probit with SEX, PCLASS, FARE, and AGE
probit <- glm(survived ~ sex.name + pclass + age + fare.distance + fare, data = train,
family = binomial(link = "probit"))
summary(probit)
###
### Saving our model and prediction as a new CSV
###
# Make our prediction on the TRAIN data set [For calculating error]
train$survived_pred <- predict(probit, train, type = "response")
train$survived_pred[train$survived_pred >= 0.5] <- 1
train$survived_pred[train$survived_pred < 0.5] <- 0
# Make a prediction with our probit on TEST
test$survived <- predict(probit, test, type = "response")
test$survived[test$survived >= 0.5] <- 1
test$survived[test$survived < 0.5] <- 0
# save csv file for submission
write.csv(test, "Submissions/probit-04.csv")
train_error(survived_pred)
randomForest(survived ~ sex + pclass + fare, data = train_new)
?ntree
model <- "randomForest(survived ~ sex + pclass + fare, data = train_new)"
model
cv_5folds <- function("model") {
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# When we change our model, we need to edit this!!!
temp_model <- randomForest(survived ~ sex + pclass + fare, data = train_new)
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_5folds <- function("model") {
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# When we change our model, we need to edit this!!!
temp_model <- randomForest(survived ~ sex + pclass + fare, data = train_new)
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_5folds <- function(model) {
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# When we change our model, we need to edit this!!!
temp_model <- randomForest(survived ~ sex + pclass + fare, data = train_new)
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
model
parse(randomForest(survived ~ sex + pclass + fare, data = train_new))
parse(model)
parse(text = model)
eval(parse(text = model))
model
?str_replace
library(stringr)
?str_replace
model
str_replace(model, "train", "train_new")
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
# Load in our models (edit this!)
source("2-randomForest.R")
str_replace(model, "data = train", "data = train_new")
model <- "randomForest(survived ~ sex.name + pclass + age + fare.distance + fare, data = train, ntree = 5000, importance = TRUE)"
load("Data/train_clean.RData")  # 891 obs
load("Data/test_clean.RData")   # 418 obs
# Load in our models (edit this!)
source("2-randomForest.R")
str_replace(model, "data = train", "data = train_new")
cv_5folds <- function(model) {
# Fix our model input
model <- str_replace(model, "data = train", "data = train_new")
# Set number of equally-sized, non-overlapping chunks
k <- 5
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# Train our model on the new, smaller training data set
temp_model <- eval(parse(text = model))
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_5folds(model)
cv_kfolds <- function(model, k = 5) {
# Fix our model input
model <- str_replace(model, "data = train", "data = train_new")
# Set number of equally-sized, non-overlapping chunks
k <- k
# Find the size of each sampled chunk
k_size <- floor(nrow(train) / k)
# Randomly sample our data set then split it up into row-unique k-chunks
k_sample <- split(sample(nrow(train)), rep(1:(nrow(train)/k_size)))
# Create an errors vector to hold our errors for each test
errors <- 0
# Perform k runs
for (i in 1:k) {
# Take one k-sample and make it the new train data set
train_new <- train[-k_sample[[i]], ]
test_new <- train[k_sample[[i]], ]
# Train our model on the train_new data set
# Train our model on the new, smaller training data set
temp_model <- eval(parse(text = model))
# Predict on the new_test data set with our model
test_new$survived_pred <- predict(temp_model, test_new)
# Find the error in our model for new_test
errors[i] <- ((length(which(test_new$survived_pred == test_new$survived))) /
length(test_new$survived)) * 100
}
# Find the average error
mean(errors)
}
cv_kfolds(model)
cv_kfolds(model, k = 2)
source("2-randomForest.R")
train_error(survived_pred)
cv_kfolds(model, k = 11)
source("3-SVM.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
# Probit
source("4-probit.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
# Probit
source("4-probit.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
source("4-probit.R")
train_error(survived_pred)
cv_kfolds(model, k = 5)
>>>>>>> edfc287d85b997e9bdd06eaa15274611ec221279
